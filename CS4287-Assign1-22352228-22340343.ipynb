{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af76fd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using downloaded and verified file: ./data/oxford_pets/images.tar.gz\n",
      "Extracting ./data/oxford_pets/images.tar.gz to ./data/oxford_pets/\n",
      "Using downloaded and verified file: ./data/oxford_pets/annotations.tar.gz\n",
      "Extracting ./data/oxford_pets/annotations.tar.gz to ./data/oxford_pets/\n",
      "Dataset loaded, sample batch shape: torch.Size([4, 3, 256, 256])\n",
      "Model params: 25,867,075\n",
      "Validation → Loss: 0.2248, mIoU: 0.7727, Dice: 0.8605\n",
      "One-epoch demo loss: 0.3032\n"
     ]
    }
   ],
   "source": [
    "# CS4287-Assign1-22352228-22340343.ipynb\n",
    "# Names and IDs: Cormac Greaney - 22352228, Jan Lawinski - 22340343\n",
    "# Date: October 2025\n",
    "# Description: Image Segmentation on Oxford-IIIT Pet Dataset using VGG16-based U-Net\n",
    "# Code runs to completion: Yes\n",
    "# Reused source: https://pytorch.org/vision/stable/models.html (for pretrained VGG16)\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# Here we have all of the imports used for our project\n",
    "# ==============================================================\n",
    "import torch, torchvision\n",
    "from torchvision.transforms import functional as TF\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torchvision.datasets.utils import download_and_extract_archive\n",
    "from PIL import Image\n",
    "import numpy as np, os, matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# Here we check for my GPU so that we can drastically reduce the runtime \n",
    "# but will revert to the cpu if its run on a machine without a GPU\n",
    "# ======================================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# Here we download the Oxford-IIIT Pet datasets images and masks\n",
    "# ==============================================================\n",
    "root = \"./data/oxford_pets/\"\n",
    "download_and_extract_archive(\n",
    "    url=\"https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\",\n",
    "    download_root=root, extract_root=root)\n",
    "download_and_extract_archive(\n",
    "    url=\"https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\",\n",
    "    download_root=root, extract_root=root)\n",
    "\n",
    "\n",
    "# =======================================================================\n",
    "# Here we create our custom dataset class for the Oxford-IIIT Pet Dataset\n",
    "# =======================================================================\n",
    "class OxfordPetSegmentation(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = [f for f in os.listdir(img_dir) if f.endswith(\".jpg\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        mask_path = os.path.join(self.mask_dir, \"trimaps\", img_name.replace(\".jpg\", \".png\"))\n",
    "\n",
    "        # === Here we load the image and mask ===\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path)\n",
    "\n",
    "        # === Here we resize the image and mask === \n",
    "        image = TF.resize(image, (256, 256), interpolation=Image.BILINEAR)\n",
    "        mask = TF.resize(mask, (256, 256), interpolation=Image.NEAREST)\n",
    "\n",
    "        # === Here we convert image to tensor and normalize the channels the match our pretrained VGG16 ===\n",
    "        image = TF.to_tensor(image)\n",
    "        image = TF.normalize(image, [0.485, 0.456, 0.406],\n",
    "                                   [0.229, 0.224, 0.225])\n",
    "        \n",
    "        # === Here we convert mask to a NumPy array ===\n",
    "        mask = np.array(mask)\n",
    "\n",
    "        # === Here we adjust the mask labels to be 0,1,2 ===\n",
    "        mask = np.clip(mask, 1, 3) - 1\n",
    "\n",
    "        # === Here we convert to a long tensor ===\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "\n",
    "        # === Here we return a tuple for training ===\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "# === Transforms - not using atm ===\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# Here we instantiate our dataset\n",
    "# ==============================================================\n",
    "dataset = OxfordPetSegmentation(\n",
    "    img_dir=os.path.join(root, \"images\"),\n",
    "    mask_dir=os.path.join(root, \"annotations\"),\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# Here we create our dataloader and wrap our dataset\n",
    "# we will use a batch size of 4 for training with shuffling turned on\n",
    "# ===================================================================\n",
    "loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# Here we pull a sample batch to verify everything is working\n",
    "# we print the shape of the batch as a sanity check\n",
    "# ==============================================================\n",
    "print(\"Dataset loaded, sample batch shape:\", next(iter(loader))[0].shape)\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# Here we define our VGG16-based U-Net model\n",
    "# ==============================================================\n",
    "class UNetVGG16(nn.Module):\n",
    "\n",
    "    # === Here we define the cunstructor with 3 classes, background, pet, border ===\n",
    "    def __init__(self, n_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # === Here we load the pretrained VGG16 model ===\n",
    "        vgg = models.vgg16_bn(weights=models.VGG16_BN_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        # === Here we extract the features layers into a python list so it can be sliced ===\n",
    "        features = list(vgg.features.children())\n",
    "\n",
    "\n",
    "        # =============================================================================================\n",
    "        # Here we create slices of VGG16 for encoder path, following the standard split we found online\n",
    "        # https://medium.com/@mygreatlearning/everything-you-need-to-know-about-vgg16-7315defb5918\n",
    "        # ============================================================================================= \n",
    "        self.enc1 = nn.Sequential(*features[:6])    # 64 filters\n",
    "        self.enc2 = nn.Sequential(*features[6:13])  # 128 filters\n",
    "        self.enc3 = nn.Sequential(*features[13:23]) # 256 filters\n",
    "        self.enc4 = nn.Sequential(*features[23:33]) # 512 filters\n",
    "        self.center = nn.Sequential(*features[33:43]) # 512 bottleneck\n",
    "\n",
    "\n",
    "        # ===============================================================\n",
    "        # Here we define the decoder path with upsampling and conv layers\n",
    "        # ===============================================================\n",
    "        self.up4 = nn.ConvTranspose2d(512, 512, kernel_size=2, stride=2)\n",
    "        self.dec4 = nn.Sequential(\n",
    "            nn.Conv2d(512 + 512, 512, 3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.Conv2d(256 + 256, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv2d(128 + 128, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(64 + 64, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.final = nn.Conv2d(64, n_classes, kernel_size=1)\n",
    "\n",
    "\n",
    "    # ===============================================================\n",
    "    # Here we define our forward pass\n",
    "    # ===============================================================\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(e1)\n",
    "        e3 = self.enc3(e2)\n",
    "        e4 = self.enc4(e3)\n",
    "        c = self.center(e4)\n",
    "\n",
    "        d4 = self.up4(c)\n",
    "        d4 = self.dec4(torch.cat([d4, e4], dim=1))\n",
    "\n",
    "        d3 = self.up3(d4)\n",
    "        d3 = self.dec3(torch.cat([d3, e3], dim=1))\n",
    "\n",
    "        d2 = self.up2(d3)\n",
    "        d2 = self.dec2(torch.cat([d2, e2], dim=1))\n",
    "\n",
    "        d1 = self.up1(d2)\n",
    "        d1 = self.dec1(torch.cat([d1, e1], dim=1))\n",
    "\n",
    "        return self.final(d1)\n",
    "\n",
    "\n",
    "# ==========================================================================\n",
    "# Here we instantiate our model and move it to the device we checked earlier\n",
    "# Hopefully a GPU!\n",
    "# ==========================================================================\n",
    "model = UNetVGG16().to(device)\n",
    "\n",
    "\n",
    "# ===============================================================================\n",
    "# Here we print the number of trainable parameters in the model as a sanity check\n",
    "# ===============================================================================\n",
    "print(f\"Model params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "\n",
    "# ================================================================================\n",
    "# Here we define our loss function - CrossEntropyLoss for multi-class segmentation\n",
    "# ================================================================================\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# ==========================================================================================\n",
    "# Here we use the more modern AdamW optimizer with decoupled weight decay for regularization\n",
    "# We set a learning rate of 3e-4 and weight decay of 1e-4\n",
    "# https://www.datacamp.com/tutorial/adamw-optimizer-in-pytorch\n",
    "# ==========================================================================================\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "# ==============================================================================================\n",
    "# Here we define a learning rate scheduler to reduce our learning rate\n",
    "# it will be halved when a monitored metric stops showing improvement within our patience period\n",
    "# ==============================================================================================\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# Here we define a simple training loop for one epoch\n",
    "# ==============================================================\n",
    "def train_one_epoch(model, loader, criterion, optimizer):\n",
    "\n",
    "    # === Here we set the model to training mode ===\n",
    "    model.train()\n",
    "\n",
    "    # === Here we initialize total loss for the epoch ===\n",
    "    total_loss = 0\n",
    "\n",
    "    # === Here we loop over the data loader ===\n",
    "    for imgs, masks in loader:\n",
    "\n",
    "        # === Here we move the images and masks to the device for ===\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "\n",
    "        # === Here we zero the gradients, perform a forward pass, compute the loss, perform a backward pass, and update weights ===\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # === Here we add the loss to the total loss ===\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # === Here we return the average loss for the epoch ===    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "# ===========================================================================\n",
    "# Here we define our helper functions for evaluating the segmentation quality\n",
    "# ===========================================================================\n",
    "def compute_iou(preds, labels, num_classes=3):\n",
    "    \"\"\"Compute mean Intersection over Union (mIoU) across classes.\"\"\"\n",
    "    ious = []\n",
    "    preds = torch.argmax(preds, dim=1)  # convert from logits to class IDs\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = (preds == cls)\n",
    "        target_inds = (labels == cls)\n",
    "        intersection = (pred_inds & target_inds).float().sum()\n",
    "        union = (pred_inds | target_inds).float().sum()\n",
    "        if union == 0:\n",
    "            ious.append(torch.tensor(float('nan')))  # ignore empty classes\n",
    "        else:\n",
    "            ious.append(intersection / union)\n",
    "    return torch.tensor(ious).nanmean().item()\n",
    "\n",
    "\n",
    "def compute_dice(preds, labels, num_classes=3):\n",
    "    \"\"\"Compute mean Dice coefficient across classes.\"\"\"\n",
    "    dice_scores = []\n",
    "    preds = torch.argmax(preds, dim=1)\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = (preds == cls)\n",
    "        target_inds = (labels == cls)\n",
    "        intersection = (pred_inds & target_inds).float().sum()\n",
    "        dice = (2. * intersection) / (pred_inds.float().sum() + target_inds.float().sum() + 1e-8)\n",
    "        dice_scores.append(dice)\n",
    "    return torch.tensor(dice_scores).nanmean().item()\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# Here we define our validation loop to measure model performance\n",
    "# ==============================================================\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()  # set to evaluation mode\n",
    "    total_loss = 0\n",
    "    total_iou = 0\n",
    "    total_dice = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in loader:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss.item()\n",
    "            total_iou += compute_iou(outputs, masks)\n",
    "            total_dice += compute_dice(outputs, masks)\n",
    "            count += 1\n",
    "\n",
    "    avg_loss = total_loss / count\n",
    "    avg_iou = total_iou / count\n",
    "    avg_dice = total_dice / count\n",
    "\n",
    "    return avg_loss, avg_iou, avg_dice\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# Here we run a demo of our one epoch function and store the loss\n",
    "# ===============================================================\n",
    "demo_loss = train_one_epoch(model, loader, criterion, optimizer)\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# Here we run a validation demo to check IoU and Dice calculations\n",
    "# ================================================================\n",
    "val_loss, val_iou, val_dice = validate(model, loader, criterion)\n",
    "print(f\"Validation → Loss: {val_loss:.4f}, mIoU: {val_iou:.4f}, Dice: {val_dice:.4f}\")\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# Here we print the demos loss to 4 decimal places\n",
    "# =============================================================\n",
    "print(f\"One-epoch demo loss: {demo_loss:.4f}\")\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# Next Steps and TODOs\n",
    "# ==============================================================\n",
    "# TODO:\n",
    "# - Implement validation loop computing IoU/Dice\n",
    "# - Add K-Fold CV using sklearn.model_selection.KFold\n",
    "# - Add ablation study cells: change LR, dropout, augmentation\n",
    "# - Save sample predictions and plot alongside ground-truth\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
